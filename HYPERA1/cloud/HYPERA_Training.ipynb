{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "First, let's make sure all required packages are installed. Run the cell below to install the necessary dependencies.\n",
    "\n",
    "**Note**: After running this cell, you'll need to restart the kernel as the cell will force a restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required dependencies\n",
    "# Run this cell first to ensure all necessary packages are installed\n",
    "\n",
    "# Essential packages\n",
    "!pip install ruptures==1.1.9  # Change point detection\n",
    "!pip install git+https://github.com/Project-MONAI/MONAI.git@7c26e5af385eb5f7a813fa405c6f3fc87b7511fa  # Medical image processing\n",
    "!pip install torch==2.7.0.dev20250221 torchvision==0.22.0.dev20250221  # Deep learning\n",
    "!pip install numpy==1.26.4 pandas==2.2.3 matplotlib==3.10.0  # Data analysis and visualization\n",
    "!pip install scikit-learn scikit-image==0.25.2 scipy==1.13.1  # Scientific computing\n",
    "!pip install tqdm tensorboard==2.19.0  # Progress and visualization\n",
    "!pip install nibabel==5.3.2  # Medical image I/O\n",
    "!pip install statsmodels==0.14.4  # Time series analysis and statistical models\n",
    "!pip install torchmetrics==1.2.1  # Additional PyTorch metrics\n",
    "\n",
    "# Optional packages\n",
    "!pip install h5py==3.13.0 SimpleITK==2.4.1 opencv-python networkx pillow\n",
    "\n",
    "# Restart the kernel to ensure changes take effect\n",
    "import os\n",
    "os.kill(os.getpid(), 9)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "First, let's make sure all required packages are installed. Run the cell below to install the necessary dependencies.\n",
    "\n",
    "**Note**: After running this cell, you'll need to restart the kernel as the cell will force a restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required dependencies\n",
    "# Run this cell first to ensure all necessary packages are installed\n",
    "\n",
    "# Essential packages\n",
    "!pip install ruptures==1.1.9  # Change point detection\n",
    "!pip install git+https://github.com/Project-MONAI/MONAI.git@7c26e5af385eb5f7a813fa405c6f3fc87b7511fa  # Medical image processing\n",
    "!pip install torch==2.7.0.dev20250221 torchvision==0.22.0.dev20250221  # Deep learning\n",
    "!pip install numpy==1.26.4 pandas==2.2.3 matplotlib==3.10.0  # Data analysis and visualization\n",
    "!pip install scikit-learn scikit-image==0.25.2 scipy==1.13.1  # Scientific computing\n",
    "!pip install tqdm tensorboard==2.19.0  # Progress and visualization\n",
    "!pip install nibabel==5.3.2  # Medical image I/O\n",
    "\n",
    "# Optional packages\n",
    "!pip install h5py==3.13.0 SimpleITK==2.4.1 opencv-python networkx pillow\n",
    "\n",
    "# Restart the kernel to ensure changes take effect\n",
    "import os\n",
    "os.kill(os.getpid(), 9)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERA Training on RunPod\n",
    "\n",
    "This notebook runs the HYPERA training with agent-based hyperparameter optimization on RunPod."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's check that we have GPU access and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages\n",
    "\n",
    "Let's make sure we have all the required packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install monai matplotlib scikit-learn scikit-image tensorboard stable-baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add HYPERA to Python Path\n",
    "\n",
    "Make sure Python can find the HYPERA modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add HYPERA to path\n",
    "hypera_path = os.path.abspath(\"HYPERA1\")\n",
    "if hypera_path not in sys.path:\n",
    "    sys.path.append(hypera_path)\n",
    "    print(f\"Added {hypera_path} to Python path\")\n",
    "\n",
    "# List the contents of the HYPERA1 directory to confirm it's there\n",
    "!ls -la HYPERA1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset\n",
    "\n",
    "Make sure the BBBC039 dataset is properly set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "if not os.path.exists(\"BBBC039\"):\n",
    "    print(\"Dataset not found. Please upload or download the BBBC039 dataset.\")\n",
    "    # Uncomment these lines to download the dataset (adjust URLs as needed)\n",
    "    !mkdir -p BBBC039/images BBBC039/masks BBBC039_metadata\n",
    "    !wget -P BBBC039/images https://data.broadinstitute.org/bbbc/BBBC039/images.zip\n",
    "    !wget -P BBBC039/masks https://data.broadinstitute.org/bbbc/BBBC039/masks.zip\n",
    "    !unzip BBBC039/images/images.zip -d BBBC039/images/\n",
    "    !unzip BBBC039/masks/masks.zip -d BBBC039/masks/\n",
    "else:\n",
    "    print(\"Dataset found. Checking contents...\")\n",
    "    !ls -la BBBC039\n",
    "    \n",
    "# Check if metadata exists\n",
    "if not os.path.exists(\"BBBC039_metadata\"):\n",
    "    print(\"Metadata not found. Creating metadata directory...\")\n",
    "    !mkdir -p BBBC039_metadata\n",
    "    \n",
    "    # Create training and validation split files\n",
    "    print(\"Creating training and validation splits...\")\n",
    "    !ls BBBC039/masks/*.png > BBBC039_metadata/all_masks.txt\n",
    "    !head -n $(( $(wc -l < BBBC039_metadata/all_masks.txt) * 80 / 100 )) BBBC039_metadata/all_masks.txt > BBBC039_metadata/training.txt\n",
    "    !tail -n $(( $(wc -l < BBBC039_metadata/all_masks.txt) * 20 / 100 )) BBBC039_metadata/all_masks.txt > BBBC039_metadata/validation.txt\n",
    "    \n",
    "    print(f\"Training samples: {!wc -l BBBC039_metadata/training.txt}\")\n",
    "    print(f\"Validation samples: {!wc -l BBBC039_metadata/validation.txt}\")\n",
    "else:\n",
    "    print(\"Metadata found. Checking contents...\")\n",
    "    !ls -la BBBC039_metadata\n",
    "    print(f\"Training samples: {!wc -l BBBC039_metadata/training.txt}\")\n",
    "    print(f\"Validation samples: {!wc -l BBBC039_metadata/validation.txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fix MONAI Error\n",
    "\n",
    "Apply the fix for the MONAI one_hot error we encountered earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we need to apply the MONAI fix\n",
    "import re\n",
    "\n",
    "train_file_path = os.path.join(hypera_path, \"legacy\", \"train_bbbc039_with_agents.py\")\n",
    "\n",
    "with open(train_file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Check if the fix is already applied\n",
    "if \"SelectItemsd(keys=[\\\"label\\\"], dim=0, index=0)\" not in content:\n",
    "    print(\"Applying MONAI fix to train_bbbc039_with_agents.py...\")\n",
    "    \n",
    "    # Replace the AsDiscreted transform in train_transforms\n",
    "    content = re.sub(\n",
    "        r'(\\s+)AsDiscreted\\(keys=\"label\", argmax=True, to_onehot=\\d+\\)',\n",
    "        r'\\1# Make sure labels have only one channel before one-hot encoding\\n\\1SelectItemsd(keys=[\"label\"], dim=0, index=0),  # Select only the first channel\\n\\1AsDiscreted(keys=\"label\", to_onehot=2, argmax=False)',\n",
    "        content\n",
    "    )\n",
    "    \n",
    "    # Replace the AsDiscreted transform in val_transforms\n",
    "    content = re.sub(\n",
    "        r'(\\s+)AsDiscreted\\(keys=\"label\", argmax=True, to_onehot=\\d+\\)',\n",
    "        r'\\1# Make sure labels have only one channel before one-hot encoding\\n\\1SelectItemsd(keys=[\"label\"], dim=0, index=0),  # Select only the first channel\\n\\1AsDiscreted(keys=\"label\", to_onehot=2, argmax=False)',\n",
    "        content\n",
    "    )\n",
    "    \n",
    "    with open(train_file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "    \n",
    "    print(\"Fix applied successfully!\")\n",
    "else:\n",
    "    print(\"MONAI fix already applied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Fix Loss Function Configuration\n",
    "\n",
    "Apply the fix for the loss function configuration to avoid double one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the loss function configuration in train_bbbc039_with_agents.py\n",
    "import re\n",
    "\n",
    "train_file_path = os.path.join(hypera_path, \"legacy\", \"train_bbbc039_with_agents.py\")\n",
    "\n",
    "with open(train_file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Check if the fix is already applied\n",
    "if \"to_onehot_y=True\" in content:\n",
    "    print(\"Applying loss function fix to train_bbbc039_with_agents.py...\")\n",
    "    \n",
    "    # Replace to_onehot_y=True with to_onehot_y=False in loss functions\n",
    "    content = re.sub(\n",
    "        r'to_onehot_y=True',\n",
    "        r'to_onehot_y=False',  # Labels are already one-hot encoded',\n",
    "        content\n",
    "    )\n",
    "    \n",
    "    with open(train_file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "    \n",
    "    print(\"Loss function fix applied successfully!\")\n",
    "else:\n",
    "    print(\"Loss function fix already applied or not needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c. Apply RunPod-Specific Fixes\n",
    "\n",
    "Apply fixes to ensure compatibility with RunPod environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to indicate we're on RunPod\n",
    "import os\n",
    "os.environ['RUNPOD_POD_ID'] = 'NOTEBOOK_ENVIRONMENT'\n",
    "print(\"Set RunPod environment variable to ensure proper DataLoader configuration\")\n",
    "\n",
    "# Check if our RunPod detection code is present\n",
    "train_file_path = os.path.join(hypera_path, \"legacy\", \"train_bbbc039_with_agents.py\")\n",
    "\n",
    "with open(train_file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "if \"is_runpod = os.environ.get('RUNPOD_POD_ID')\" not in content:\n",
    "    print(\"WARNING: RunPod detection code not found in training script.\")\n",
    "    print(\"The script may not be configured for RunPod environment.\")\n",
    "    print(\"Please check the latest version of the code.\")\n",
    "else:\n",
    "    print(\"RunPod detection code found in training script.\")\n",
    "    print(\"DataLoader will use 0 worker processes to avoid multiprocessing issues.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Fix Loss Function Configuration\n",
    "\n",
    "Apply the fix for the loss function configuration to avoid double one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the loss function configuration in train_bbbc039_with_agents.py\n",
    "import re\n",
    "\n",
    "train_file_path = os.path.join(hypera_path, \"legacy\", \"train_bbbc039_with_agents.py\")\n",
    "\n",
    "with open(train_file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Check if the fix is already applied\n",
    "if \"to_onehot_y=True\" in content:\n",
    "    print(\"Applying loss function fix to train_bbbc039_with_agents.py...\")\n",
    "    \n",
    "    # Replace to_onehot_y=True with to_onehot_y=False in loss functions\n",
    "    content = re.sub(\n",
    "        r'to_onehot_y=True',\n",
    "        r'to_onehot_y=False',  # Labels are already one-hot encoded',\n",
    "        content\n",
    "    )\n",
    "    \n",
    "    with open(train_file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "    \n",
    "    print(\"Loss function fix applied successfully!\")\n",
    "else:\n",
    "    print(\"Loss function fix already applied or not needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Fix Loss Function Configuration\n",
    "\n",
    "Apply the fix for the loss function configuration to avoid double one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the loss function configuration in train_bbbc039_with_agents.py\n",
    "import re\n",
    "\n",
    "train_file_path = os.path.join(hypera_path, \"legacy\", \"train_bbbc039_with_agents.py\")\n",
    "\n",
    "with open(train_file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Check if the fix is already applied\n",
    "if \"to_onehot_y=True\" in content:\n",
    "    print(\"Applying loss function fix to train_bbbc039_with_agents.py...\")\n",
    "    \n",
    "    # Replace to_onehot_y=True with to_onehot_y=False in loss functions\n",
    "    content = re.sub(\n",
    "        r'to_onehot_y=True',\n",
    "        r'to_onehot_y=False',  # Labels are already one-hot encoded',\n",
    "        content\n",
    "    )\n",
    "    \n",
    "    with open(train_file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "    \n",
    "    print(\"Loss function fix applied successfully!\")\n",
    "else:\n",
    "    print(\"Loss function fix already applied or not needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Training with Agent-Based Hyperparameter Optimization\n",
    "\n",
    "Now let's run the training with the agent-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HYPERA training module\n",
    "from legacy.train_bbbc039_with_agents import main\n",
    "\n",
    "# Run training with agent-based hyperparameter optimization\n",
    "main(\n",
    "    experiment_type=\"agent_factory\",  # Use the agent-based approach\n",
    "    epochs=100,                       # Number of epochs\n",
    "    batch_size=16,                    # Batch size\n",
    "    early_stopping=20,                # Early stopping patience\n",
    "    use_cloud=False                   # Don't use cloud storage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results with TensorBoard\n",
    "\n",
    "Use TensorBoard to visualize the training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare with No-Agent Approach (Optional)\n",
    "\n",
    "Run training without agents to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training without agents (using fixed hyperparameters)\n",
    "# Uncomment to run\n",
    "'''\n",
    "main(\n",
    "    experiment_type=\"no_agent\",  # Use fixed hyperparameters\n",
    "    epochs=100,                  # Number of epochs\n",
    "    batch_size=16,               # Batch size\n",
    "    early_stopping=20,           # Early stopping patience\n",
    "    use_cloud=False              # Don't use cloud storage\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze and Compare Results\n",
    "\n",
    "Compare the performance of the agent-based approach vs. fixed hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of training curves\n",
    "# This is just a placeholder - you'll need to adapt this to your actual results\n",
    "'''\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results from CSV files (adjust paths as needed)\n",
    "agent_results = pd.read_csv('results_agent/metrics.csv')\n",
    "no_agent_results = pd.read_csv('results_no_agent/metrics.csv')\n",
    "\n",
    "# Plot Dice scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(agent_results['epoch'], agent_results['val_dice'], label='Agent-Based')\n",
    "plt.plot(no_agent_results['epoch'], no_agent_results['val_dice'], label='Fixed Hyperparameters')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Dice Score')\n",
    "plt.title('Agent-Based vs. Fixed Hyperparameters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save and Download Results\n",
    "\n",
    "Make sure to save and download your results before stopping the pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress results for easy download\n",
    "!tar -czvf hypera_results.tar.gz runs/ results_*/ models/\n",
    "print(\"Results compressed to hypera_results.tar.gz\")\n",
    "print(\"Download this file using the Jupyter file browser.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}